{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Question3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import sys\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from datasets import Dataset\n",
        "from helper import HelperFunctions\n",
        "from params import RunningParams\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "RunningParams = RunningParams()\n",
        "Dataset = Dataset()\n",
        "HelperFunctions = HelperFunctions()\n",
        "\n",
        "layer = 4\n",
        "\n",
        "model = torchvision.models.resnet50(pretrained=True).eval()\n",
        "feature_extractor = nn.Sequential(*list(model.children())[: layer - 6]).cuda()\n",
        "\n",
        "# ---------- Parse argv -------------\n",
        "val_datasets = sys.argv[1]\n",
        "inat = (sys.argv[2] == 'True')\n",
        "RunningParams.INAT = inat\n",
        "if val_datasets == Dataset.CUB200 and RunningParams.INAT is True:\n",
        "    RunningParams.Deformable_ProtoPNet = True\n",
        "\n",
        "if RunningParams.Deformable_ProtoPNet:\n",
        "    from cub200_features import get_resnet50_features\n",
        "\n",
        "    model = get_resnet50_features(inat=RunningParams.INAT, pretrained=True)\n",
        "\n",
        "    model = model.cuda()\n",
        "    model.eval()\n",
        "    model = nn.DataParallel(model)\n",
        "    feature_extractor = model\n",
        "else:\n",
        "    if RunningParams.AP_FEATURE:\n",
        "        feature_extractor = nn.Sequential(*list(model.children())[:-1]).cuda()\n",
        "    model.cuda()\n",
        "\n",
        "    feature_extractor = nn.DataParallel(feature_extractor)\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "imagenet_train_data = ImageFolder(\n",
        "    # ImageNet train folder\n",
        "    root=\"/home/giang/Downloads/train/\", transform=Dataset.imagenet_transform\n",
        ")\n",
        "\n",
        "# print(RunningParams.__dict__)\n",
        "\n",
        "for val_dataset in [val_datasets]:\n",
        "    if val_dataset == Dataset.CUB200:\n",
        "        imagenet_train_data = ImageFolder(\n",
        "            # CUB train folder\n",
        "            root=\"/home/giang/Downloads/cub200/train/\",\n",
        "            transform=Dataset.imagenet_transform,\n",
        "        )\n",
        "        RunningParams.DEEP_NN_TEST = False\n",
        "\n",
        "    random.seed(42)\n",
        "    np.random.seed(42)\n",
        "    print('Running {} ...'.format(val_dataset))\n",
        "\n",
        "    if (\n",
        "        RunningParams.IMAGENET_REAL is True\n",
        "    ):\n",
        "        real_json = open(\"reassessed-imagenet/real.json\")\n",
        "        real_ids = json.load(real_json)\n",
        "        real_labels = {\n",
        "            f\"ILSVRC2012_val_{i + 1:08d}.JPEG\": labels\n",
        "            for i, labels in enumerate(real_ids)\n",
        "        }\n",
        "\n",
        "    # Load validation datasets\n",
        "    if val_dataset in Dataset.IMAGENET_C_NOISE:\n",
        "        imagenet_val_data = ImageFolder(\n",
        "            root=\"/home/giang/Downloads/shared_datasets/imagenet-c/{}/\".format(\n",
        "                val_dataset\n",
        "            ),\n",
        "            transform=Dataset.imagenet_transform,\n",
        "        )\n",
        "    else:\n",
        "        if (\n",
        "            val_dataset == Dataset.ADVERSARIAL_PATCH_NEW\n",
        "            or val_dataset == Dataset.DAMAGE_NET\n",
        "        ):\n",
        "            imagenet_val_data = ImageFolder(\n",
        "                root=\"/home/giang/Downloads/{}/\".format(val_dataset),\n",
        "                transform=Dataset.imagenet_transform_crop_patch,\n",
        "            )\n",
        "        else:\n",
        "            if val_dataset == Dataset.CUB200:\n",
        "                imagenet_val_data = ImageFolder(\n",
        "                    root=\"/home/giang/Downloads/cub200/{}/\".format(val_dataset),\n",
        "                    transform=Dataset.imagenet_transform,\n",
        "                )\n",
        "            else:\n",
        "                if val_dataset in [Dataset.IMAGENET_1K_50K_CLEAN, Dataset.OBJECTNET_5K, Dataset.IMAGENET_A]:\n",
        "                    imagenet_val_data = ImageFolder(\n",
        "                        root=\"/home/giang/Downloads/shared_datasets/{}/\".format(val_dataset),\n",
        "                        transform=Dataset.imagenet_transform,\n",
        "                    )\n",
        "                else:\n",
        "                    imagenet_val_data = ImageFolder(\n",
        "                        root=\"/home/giang/Downloads/{}/\".format(val_dataset),\n",
        "                        transform=Dataset.imagenet_transform,\n",
        "                    )\n",
        "\n",
        "    if val_dataset in (\n",
        "        [\n",
        "            Dataset.IMAGENET_A,\n",
        "            Dataset.IMAGENET_R,\n",
        "            Dataset.OBJECTNET_5K,\n",
        "            Dataset.IMAGENET_HARD,\n",
        "            Dataset.IMAGENET_MULTI_OBJECT,\n",
        "            Dataset.IMAGENET_PILOT_VIS,\n",
        "        ]\n",
        "        + Dataset.IMAGENET_C_NOISE\n",
        "    ):  # Subset the train dataset\n",
        "        imagenet_train_indices = [\n",
        "            i\n",
        "            for i in range(len(imagenet_train_data))\n",
        "            if HelperFunctions.train_extract_wnid(imagenet_train_data.imgs[i][0])\n",
        "            in imagenet_val_data.classes\n",
        "        ]\n",
        "        imagenet_train_set = torch.utils.data.Subset(\n",
        "            imagenet_train_data, imagenet_train_indices\n",
        "        )\n",
        "\n",
        "    if len(imagenet_val_data) < 5000 or val_dataset in [\n",
        "        Dataset.IMAGENET_MULTI_OBJECT,\n",
        "        Dataset.IMAGENET_PILOT_VIS,\n",
        "    ]:\n",
        "        N_test = len(imagenet_val_data)\n",
        "    elif val_dataset == Dataset.IMAGENET_1K_50K:\n",
        "        N_test = 50000\n",
        "    elif val_dataset == Dataset.IMAGENET_1K_50K_CLEAN:\n",
        "        N_test = 46043\n",
        "    elif val_dataset == Dataset.CUB200:\n",
        "        N_test = 5794\n",
        "    else:\n",
        "        N_test = len(imagenet_val_data)\n",
        "\n",
        "    # N_test = 4 # Just for test, remove to get the paper numbers\n",
        "\n",
        "    # Subset the validation dataset\n",
        "    random_indices = random.sample(range(0, len(imagenet_val_data)), N_test)\n",
        "    val_set_tenth = torch.utils.data.Subset(imagenet_val_data, random_indices)\n",
        "\n",
        "    bs = 512  # Batch Size\n",
        "\n",
        "    # Load only the training samples of the targeted classes (e.g. 200 classes for ImageNet-A).\n",
        "    if val_dataset in (\n",
        "        [\n",
        "            Dataset.IMAGENET_A,\n",
        "            Dataset.IMAGENET_R,\n",
        "            Dataset.OBJECTNET_5K,\n",
        "            Dataset.IMAGENET_HARD,\n",
        "            Dataset.IMAGENET_MULTI_OBJECT,\n",
        "            Dataset.IMAGENET_PILOT_VIS,\n",
        "        ]\n",
        "        + Dataset.IMAGENET_C_NOISE\n",
        "    ):\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            imagenet_train_set,\n",
        "            batch_size=bs,\n",
        "            shuffle=False,\n",
        "            num_workers=8,\n",
        "            pin_memory=True,\n",
        "        )\n",
        "    else:\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            imagenet_train_data,\n",
        "            batch_size=bs,\n",
        "            shuffle=False,\n",
        "            num_workers=8,\n",
        "            pin_memory=True,\n",
        "        )\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        val_set_tenth, batch_size=bs, shuffle=False, num_workers=8, pin_memory=True\n",
        "    )\n",
        "\n",
        "    print(f\"Total Images: {len(train_loader.dataset)} in Training folder\")\n",
        "\n",
        "    print(f\"Total Images: {len(test_loader.dataset)} in Validation folder\")\n",
        "    print(f\"Number of Classes: {len(test_loader.dataset.dataset.classes)}\")\n",
        "\n",
        "    # This class makes a dataset compatible with the original ImageNet1K dataset e.g. the indexing of classes\n",
        "    class DatasetCompat:\n",
        "        def __init__(self, train_loader, val_loader):\n",
        "            if len(train_loader.dataset.classes) < len(val_loader.dataset.classes):\n",
        "                raise ValueError(\"Validation set is not a subset of train set.\")\n",
        "            if not set(val_loader.dataset.classes) <= set(train_loader.dataset.classes):\n",
        "                raise ValueError(\"Validation set is not a subset of train set.\")\n",
        "\n",
        "            self.train_loader = train_loader\n",
        "            self.val_loader = val_loader\n",
        "\n",
        "            self.train_indices = self.train_loader.dataset.class_to_idx\n",
        "            self.train_num_class = len(train_loader.dataset.classes)\n",
        "\n",
        "            self.val_indices = self.val_loader.dataset.class_to_idx\n",
        "            self.val_num_class = len(val_loader.dataset.classes)\n",
        "\n",
        "        # Convert ImageNet1K ID to the target dataset ID (e.g. 986 to 89)\n",
        "        def convert_train_to_val_idx(self, train_idx):\n",
        "            wnid = [k for (k, v) in self.train_indices.items() if v == train_idx][0]\n",
        "            if wnid in self.val_indices:\n",
        "                val_idx = self.val_indices[wnid]\n",
        "            else:\n",
        "                val_idx = -1\n",
        "            return val_idx\n",
        "\n",
        "        # Convert the target dataset ID to ImageNet1K ID (e.g. 89 to 986)\n",
        "        def convert_val_to_train_idx(self, val_idx):\n",
        "            wnid = [k for (k, v) in self.val_indices.items() if v == val_idx][0]\n",
        "            train_idx = self.train_indices[wnid]\n",
        "            return train_idx\n",
        "\n",
        "    if val_dataset in (\n",
        "        [\n",
        "            Dataset.IMAGENET_A,\n",
        "            Dataset.IMAGENET_R,\n",
        "            Dataset.OBJECTNET_5K,\n",
        "            Dataset.IMAGENET_HARD,\n",
        "            Dataset.IMAGENET_MULTI_OBJECT,\n",
        "            Dataset.IMAGENET_PILOT_VIS,\n",
        "        ]\n",
        "        + Dataset.IMAGENET_C_NOISE\n",
        "    ):\n",
        "        data_compat = DatasetCompat(imagenet_train_set, val_set_tenth)\n",
        "    else:\n",
        "        data_compat = DatasetCompat(train_loader, val_set_tenth)\n",
        "\n",
        "    if RunningParams.DEEP_NN_TEST:\n",
        "        dataset_wnids = val_set_tenth.dataset.classes\n",
        "        mask = [wnid in dataset_wnids for wnid in Dataset.all_wnids]\n",
        "\n",
        "        correct_ones = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (data, target) in enumerate(tqdm(test_loader)):\n",
        "                data = data.cuda()\n",
        "                target_c = target.cuda()\n",
        "\n",
        "                labels = HelperFunctions.to_np(target)\n",
        "                out = model(data)\n",
        "                model_output = out[:, mask]\n",
        "                pred = model_output.data.max(1)[1]\n",
        "                correct_ones += pred.eq(target_c.data).sum().item()\n",
        "\n",
        "        acc = 100 * correct_ones / N_test\n",
        "        print(\"ResNet-50: {} Accuracy (%):\".format(val_dataset), round(acc, 4))\n",
        "        print(\"############################################################\")\n",
        "\n",
        "    all_val_embds = []\n",
        "    all_val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(tqdm(test_loader)):\n",
        "            data = data.cuda()\n",
        "            if RunningParams.Deformable_ProtoPNet:\n",
        "                embeddings = HelperFunctions.to_np(\n",
        "                    F.avg_pool2d(feature_extractor(data), (7, 7))\n",
        "                )\n",
        "            else:\n",
        "                embeddings = HelperFunctions.to_np(feature_extractor(data))\n",
        "            labels = HelperFunctions.to_np(target)\n",
        "\n",
        "            # For datasets having < 1000 classes, we need to convert its label to ImageNet1K labels\n",
        "            if val_dataset in (\n",
        "                [\n",
        "                    Dataset.IMAGENET_A,\n",
        "                    Dataset.IMAGENET_R,\n",
        "                    Dataset.OBJECTNET_5K,\n",
        "                    Dataset.IMAGENET_HARD,\n",
        "                    Dataset.IMAGENET_MULTI_OBJECT,\n",
        "                    Dataset.IMAGENET_PILOT_VIS,\n",
        "                ]\n",
        "                + Dataset.IMAGENET_C_NOISE\n",
        "            ):\n",
        "                for i in range(len(labels)):\n",
        "                    labels[i] = data_compat.convert_val_to_train_idx(labels[i])\n",
        "\n",
        "            all_val_embds.append(embeddings)\n",
        "            all_val_labels.append(labels)\n",
        "\n",
        "    all_val_concatted = HelperFunctions.concat(all_val_embds)\n",
        "    all_val_labels_concatted = HelperFunctions.concat(all_val_labels)\n",
        "\n",
        "    #\n",
        "    if RunningParams.AP_FEATURE:\n",
        "        all_val_concatted = all_val_concatted.reshape(-1, 2048)\n",
        "    else:\n",
        "        all_val_concatted = all_val_concatted.reshape(-1, 2048 * 7 * 7)\n",
        "\n",
        "    Query = torch.from_numpy(all_val_concatted)\n",
        "    Query = Query.cuda()\n",
        "    Query = F.normalize(Query, dim=1)\n",
        "\n",
        "    # Query must be shape (BatchSize, 100352) 100352=2048*7*7\n",
        "    # Query must be normalized to have unit norm\n",
        "\n",
        "    saved_results = []\n",
        "    target_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(tqdm(train_loader)):\n",
        "            # if batch_idx == 2:\n",
        "            #     break\n",
        "            data = data.cuda()\n",
        "            labels = HelperFunctions.to_np(target)\n",
        "\n",
        "            if RunningParams.Deformable_ProtoPNet:\n",
        "                embeddings = F.avg_pool2d(feature_extractor(data), (7, 7))\n",
        "            else:\n",
        "                embeddings = feature_extractor(data)\n",
        "\n",
        "            if RunningParams.AP_FEATURE:\n",
        "                embeddings = embeddings.view(-1, 2048)\n",
        "            else:\n",
        "                embeddings = embeddings.view(-1, 2048 * 7 * 7)\n",
        "            embeddings = F.normalize(embeddings, dim=1)\n",
        "            q_results = torch.einsum(\"id,jd->ij\", Query, embeddings).to(\"cpu\")\n",
        "\n",
        "            saved_results.append(q_results)\n",
        "            target_labels.append(target)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    labels_np = torch.cat(target_labels, -1)\n",
        "    val_labels_np = np.concatenate(all_val_labels)\n",
        "\n",
        "    saved_results = torch.cat(saved_results, 1)\n",
        "\n",
        "    # Compute the top-1 accuracy of KNNs, save the KNN dictionary\n",
        "    scores = {}\n",
        "    import torch\n",
        "\n",
        "    K_values = [20]\n",
        "\n",
        "    for K in K_values:\n",
        "        print(val_dataset)\n",
        "        correct_cnt = 0\n",
        "        duplicate_cnt = 0\n",
        "        for i in tqdm(range(N_test)):\n",
        "            concat_ts = saved_results[i].cuda()\n",
        "            sorted_ts = torch.argsort(concat_ts).cuda()\n",
        "            sorted_1k = sorted_ts[-50:].cuda()\n",
        "            sorted_topk = sorted_ts[-K:]\n",
        "            scores[i] = torch.flip(\n",
        "                sorted_topk, dims=[0]\n",
        "            )  # Move the closest to the head\n",
        "\n",
        "            gt_id = val_labels_np[i]\n",
        "\n",
        "            prediction = torch.argmax(torch.bincount(labels_np[scores[i]]))\n",
        "            img_name = os.path.basename(Query[0])\n",
        "            if RunningParams.IMAGENET_REAL is True:\n",
        "                if prediction in real_labels[img_name]:\n",
        "                    correctness = True\n",
        "                else:\n",
        "                    correctness = False\n",
        "            else:\n",
        "                if prediction == gt_id:\n",
        "                    correctness = True\n",
        "                else:\n",
        "                    correctness = False\n",
        "\n",
        "            if correctness:\n",
        "                correct_cnt += 1\n",
        "\n",
        "        acc = 100 * correct_cnt / N_test\n",
        "\n",
        "        print(\"The accuracy of kNN at K = {} is {}\".format(K, acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "YUn9e-TG1smC",
        "outputId": "159e4c98-67e0-4465-829a-c39434a4a6aa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d59edca36496>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhelper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHelperFunctions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRunningParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}